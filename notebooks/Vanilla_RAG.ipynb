{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DekPBYri2nK6",
        "outputId": "be44a5b7-76c9-4814-fde9-84caa6b19291"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "#!pip install langchain_community langchainhub chromadb langchain langchain-openai\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "from langchain import hub\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Using weburl as input data\n",
        "loader = WebBaseLoader(web_paths=[\"https://en.wikipedia.org/wiki/Kaggle\"])\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Splitting into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50) #Recurssive splitter for dense context splitting\n",
        "text_chunks = text_splitter.split_documents(documents)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Embedding with HuggingFace model\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = Chroma.from_documents(text_chunks, embedding=embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#Using vectorstore as retriever\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "template = \"\"\"\n",
        "You are a question answering assistant.\n",
        "\n",
        "ONLY use the information provided in the context to answer the question.\n",
        "If the context does not contain the answer, reply exactly: \"I don't know\".\n",
        "\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "#Prompt template created for model to not utitlize its pretrained data\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"context\", \"question\"],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#LLM (local HuggingFace model)\n",
        "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", max_length=512, truncation=True)\n",
        "llm = HuggingFacePipeline(pipeline=generator)\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "#RAG pipeline\n",
        "rag_pipeline = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JObYFpwB5tGh",
        "outputId": "b75b2b24-b601-4070-8768-0a4e5ba1d7cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kaggle enables users to find and publish datasets, explore and build models in a web-based data science environment, work with other data scientists and machine learning engineers, and enter competitions to solve data science challenges.\n"
          ]
        }
      ],
      "source": [
        "print(rag_pipeline.invoke(\"What is the use of Kaggle\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJwpjPoV8ahD",
        "outputId": "54ca7b2b-701d-4a44-9617-7a86cdbf9ba2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User can write and execute code in Python or R, import datasets, use popular libraries, and train models on CPUs, GPUs, or TPUs directly in the cloud.\n"
          ]
        }
      ],
      "source": [
        "print(rag_pipeline.invoke(\"what is python\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQC7okxT9OVp",
        "outputId": "82d92d6f-3002-4011-a1d2-4a41c0021c4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I don't know\n"
          ]
        }
      ],
      "source": [
        "print(rag_pipeline.invoke(\"how to transfer money\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqWI0vcNHp4_",
        "outputId": "28632a4a-3fc9-4abb-ee9f-61573be20c2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "April 2010\n"
          ]
        }
      ],
      "source": [
        "print(rag_pipeline.invoke(\"When was Kaggle launched?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3oCC_HzICkB",
        "outputId": "ce71bed1-ac76-4cf0-ef01-db80f3bc6bcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I don't know\n"
          ]
        }
      ],
      "source": [
        "print(rag_pipeline.invoke(\"When was Java launched?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m64MplGQIH2f",
        "outputId": "7098036d-9500-4e52-cfa4-d19feb4178b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I don't know.\n"
          ]
        }
      ],
      "source": [
        "print(rag_pipeline.invoke(\"What is oops concept?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VOinUy6IQd3",
        "outputId": "efd85c89-fdba-49f7-b184-90f76db7e683"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kaggle enables users to find and publish datasets, explore and build models in a web-based data science environment, work with other data scientists and machine learning engineers, and enter competitions to solve data science challenges.\n"
          ]
        }
      ],
      "source": [
        "print(rag_pipeline.invoke(\"why Kaggle was so famous?\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
