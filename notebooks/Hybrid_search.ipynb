{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcuVzuxNeGBQ",
        "outputId": "c34dd0e5-cee3-4fae-d9c7-fde904a90a65"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "from langchain import hub\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jojNiyxPeHqf"
      },
      "outputs": [],
      "source": [
        "# 1. Using weburl as input data\n",
        "loader = WebBaseLoader(web_paths=[\"https://en.wikipedia.org/wiki/Kaggle\"])\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66y2W9xfeNL_"
      },
      "outputs": [],
      "source": [
        "# 2. Splitting into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50) #Recurssive splitter for dense context splitting\n",
        "text_chunks = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4Qtl3zze8sP",
        "outputId": "4c49f42f-26f2-40f6-8efb-8e8d7fcd5933"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3375891832.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# 3. Embedding with HuggingFace model\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = Chroma.from_documents(text_chunks, embedding=embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h81Oaeo5gfUW",
        "outputId": "1928ec82-2386-4eb5-ca14-25d559acd8c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "#4. Using vectorstore as retriever\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "#retriever = vectorstore.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": 3})\n",
        "\n",
        "template = \"\"\"\n",
        "You are a QA assistant.\n",
        "Answer only using the provided context.\n",
        "if context is irrelevant to question, reply \"I don't know\".\n",
        "If multiple numbers or facts are present, prefer the most recent one.\n",
        "\n",
        "\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "#5. Prompt template created for model to not utitlize its pretrained data\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"context\", \"question\"],\n",
        ")\n",
        "\n",
        "# 5. LLM (local HuggingFace model)\n",
        "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", max_length=512, truncation=True)\n",
        "llm = HuggingFacePipeline(pipeline=generator)\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\".join(doc.page_content for doc in docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "176MUnTxfR1p"
      },
      "outputs": [],
      "source": [
        "# Convert your text chunks into plain text for BM25\n",
        "corpus = [doc.page_content for doc in text_chunks]\n",
        "\n",
        "# Tokenize\n",
        "vectorizer = CountVectorizer().build_tokenizer()\n",
        "tokenized_corpus = [vectorizer(doc) for doc in corpus]\n",
        "\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "# Function to get top k sparse results\n",
        "def bm25_retriever(query, k=3):\n",
        "    tokenized_query = vectorizer(query)\n",
        "    doc_scores = bm25.get_scores(tokenized_query)\n",
        "    top_indices = doc_scores.argsort()[-k:][::-1]\n",
        "    top_docs = [text_chunks[i] for i in top_indices]\n",
        "    return top_docs\n",
        "\n",
        "\n",
        "def hybrid_retriever(query,retriever,k=3, alpha=0.5):\n",
        "    \"\"\"\n",
        "    alpha: weight for dense vs sparse\n",
        "    \"\"\"\n",
        "    # Get dense results\n",
        "    dense_docs = retriever.get_relevant_documents(query)\n",
        "    # Get sparse results\n",
        "    sparse_docs = bm25_retriever(query, k=k)\n",
        "\n",
        "    # Merge & deduplicate (you can tune merging strategy)\n",
        "    combined_docs = {doc.page_content: doc for doc in dense_docs + sparse_docs}\n",
        "    merged_docs = list(combined_docs.values())[:k]  # keep top k\n",
        "    return merged_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGASLEUJgWxI"
      },
      "outputs": [],
      "source": [
        "rag_pipeline = (\n",
        "    {\"context\": lambda q: format_docs(hybrid_retriever(q,retriever)), \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liST-Dg6g5fY",
        "outputId": "2cde0ad4-71f4-45b9-9b72-79ce3e5886d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kaggle enables users to find and publish datasets, explore and build models in a web-based data science environment, work with other data scientists and machine learning engineers, and enter competitions to solve data science challenges.\n"
          ]
        }
      ],
      "source": [
        "print(rag_pipeline.invoke(\"What is the use of Kaggle\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sK7bc-mChGLG",
        "outputId": "2e7d0d94-c183-4470-fc36-3242b19650a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I don't know\n"
          ]
        }
      ],
      "source": [
        "print(rag_pipeline.invoke(\"what is python\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MizhL0yYhKkG",
        "outputId": "654dd6f3-939c-4ca2-ae82-1ff0c0ea5242"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I don't know\n"
          ]
        }
      ],
      "source": [
        "print(rag_pipeline.invoke(\"how to transfer money\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgnWG29shRvG",
        "outputId": "1bcd7419-500d-4218-e27d-27b45c74cd2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "April 2010\n"
          ]
        }
      ],
      "source": [
        "print(rag_pipeline.invoke(\"When was Kaggle launched?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2sJ2YAKhS_-",
        "outputId": "a47b5607-03f6-49fc-cfd7-d943d4ae447f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I don't know\n"
          ]
        }
      ],
      "source": [
        "print(rag_pipeline.invoke(\"When was Java launched?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jK_YHKBEhWdG",
        "outputId": "bad06d62-c0bb-4291-8565-256475b458a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I don't know\n"
          ]
        }
      ],
      "source": [
        "print(rag_pipeline.invoke(\"What is oops concept?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHT1Yd8rhi_d",
        "outputId": "60c7dcb4-aa5c-4498-e31f-8435be8909ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kaggle was founded by Anthony Goldbloom in April 2010.[2] Jeremy Howard, one of the first Kaggle users, joined in November 2010 and served as the President and Chief Scientist.[3] Also on the team was Nicholas Gruen serving as the founding chair.[4] In 2011, the company raised $12.5 million and Max Levchin became the chairman.[5] On March 8, 2017, Fei-Fei Li, Chief Scientist at Google, announced that Google was acquiring Kaggle.[6] Kaggle is a data science competition platform and online community for data scientists and machine learning practitioners under Google LLC.\n"
          ]
        }
      ],
      "source": [
        "print(rag_pipeline.invoke(\"why Kaggle was so famous?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKulXWNVNQBk",
        "outputId": "d37abbf8-742c-4210-9804-43d528d0fdf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The highest tier, Kaggle Grandmaster, is awarded to users who have ranked at the top of multiple competitions including high ranking in a solo team.\n"
          ]
        }
      ],
      "source": [
        "print(rag_pipeline.invoke(\"What is a Kaggle Grandmaster?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JP5XqtnwRfSz",
        "outputId": "76d76e8c-adfd-41a4-822b-8790301144a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15 million\n"
          ]
        }
      ],
      "source": [
        "print(rag_pipeline.invoke(\"How many Kaggle users are there?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uN21mWwgRvSK",
        "outputId": "ed394574-9007-489c-f6b2-0c9d52e5b6f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 million registered users, and as of October 2023, it has over 15 million users in 194 countries.\n"
          ]
        }
      ],
      "source": [
        "print(rag_pipeline.invoke(\"how many people using kaggle\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
