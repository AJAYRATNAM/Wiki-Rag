Retrieval Quality Metrics

How well the retriever finds relevant context.

Precision@K / Recall@K â†’ Fraction of retrieved documents that are relevant (precision) vs fraction of relevant documents retrieved (recall).
Example: If only 2 of top-5 chunks are relevant, Precision@5 = 0.4.

Mean Reciprocal Rank (MRR) â†’ Measures how high the first relevant document appears in the ranked list.

Normalized Discounted Cumulative Gain (nDCG) â†’ Accounts for ranking quality by giving higher weight to relevant docs appearing earlier.

ğŸ”¹ 2. Answer Quality Metrics

How good the generated response is.

Faithfulness (Groundedness) â†’ Is the answer supported by retrieved evidence (avoids hallucinations)?

Answer Relevancy (Correctness) â†’ Does the answer address the userâ€™s query meaningfully?

Factual Consistency â†’ Matches ground-truth facts (can be evaluated via NLI models).

Completeness â†’ Does the answer cover all necessary parts of the query?

ğŸ”¹ 3. Context Utilization Metrics

How well the LLM used the retrieved documents.

Context Recall â†’ % of relevant ground-truth facts covered in the answer.

Context Precision â†’ % of answer content actually supported by retrieved context.

Context Utilization Rate â†’ How often retrieved context actually influences the final answer.

ğŸ”¹ 4. User-Centric / Utility Metrics

How the system feels in practice.

Helpfulness / Usefulness (LLM-as-judge or human eval) â†’ Would the answer help a real user?

Readability / Fluency â†’ Grammar, style, coherence.

Toxicity / Bias â†’ Is the response safe and unbiased?

Latency / Efficiency â†’ Time taken to generate an answer.

ğŸ”¹ 5. System-Level Metrics

For production RAG evaluation.

Coverage â†’ % of queries where at least one relevant doc is retrieved.

Answerability â†’ % of queries where the system should return â€œI donâ€™t knowâ€ but doesnâ€™t hallucinate.

Calibration â†’ Does model express uncertainty correctly (confidence vs correctness)?

Cost Efficiency â†’ Tokens used per query (retrieval + generation).

âœ… Interview Tip:
If asked â€œHow would you evaluate a RAG system?â€, structure your answer in layers:

Retrieval quality â†’ Precision, Recall, MRR, nDCG.

Generation quality â†’ Faithfulness, relevancy, factual consistency, completeness.

Context usage â†’ Context recall/precision, utilization.

User/system metrics â†’ Helpfulness, safety, latency, cost.

This shows you think end-to-end instead of just â€œanswer looks goodâ€.